---
title: "13. Inference for one proportion"
author: "Put your name here"
date: "Put the date here"
output:
    html_notebook:
        toc: yes
        toc_float: yes
---

<!-- Please don't mess with the next few lines! -->
<style>h5{font-size:2em;color:#0000FF}h6{font-size:1.5em;color:#0000FF}div.answer{margin-left:5%;border:1px solid #0000FF;border-left-width:10px;padding:25px} div.summary{background-color:rgba(30,144,255,0.1);border:3px double #0000FF;padding:25px}</style>`r options(scipen=999)`<p style="color:#ffffff">`r intToUtf8(c(49,46,50))`</p>
<!-- Please don't mess with the previous few lines! -->

<div class = "summary">
### Functions introduced in this module:
`binom.test`
</div>


## Introduction

Our earlier work with simulations showed us that when the number of successes and failures is large enough, we can use a normal model as our sampling distribution model.

We revisit hypothesis tests for a single proportion, but now, instead of running a simulation to compute the P-value, we take the shortcut of computing the P-value directly from a normal model.

There are no new concepts here. All we are doing is revisiting the rubric for inference and making the necessary changes.


## Load packages

We load the standard `mosaic` package as well as the `broom` package for tidy output and the `openintro` package to access data on heart transplant candidates.

```{r, message = FALSE}
library(openintro)
library(broom)
library(mosaic)
```


## Revisiting the rubric for inference

Instead of running a simulation, we are going to assume that the sampling distribution can be modeled with a normal model as long as the conditions for using a normal model are met.

Although the rubric has not changed, the use of a normal model changes quite a bit about the way we go through the other steps. For example, we won't have simulated values to give us a histogram of the null model. Instead, we'll go straight to graphing a normal model. We won't compute the percent of our simulated samples that are at least as extreme as our test statistic to get the P-value. The P-value from a normal model is found directly from the model itself using the `pdist` command.

What follows is a fully-worked example of inference for one proportion. After the hypothesis test (sometimes called a one-proportion z-test for reasons that will become clear), we also follow up by computing a confidence interval. **From now on, we will consider inference to consist of a hypothesis test and a confidence interval.** Whenever you're asked a question that requires statistical inference, you should follow both the rubric steps for a hypothesis test and for a confidence interval.

The example below will pause frequently for commentary on the steps, especially where their execution will be different from what you've seen before when you used simulation. When it's your turn to work through another example on your own, you should follow the outline of the rubric, but you should **not** copy and paste the commentary that accompanies it.


## Research question

Data from the Stanford University Heart Transplant Study is located in the `openintro` package in a data frame called `heartTr`. From the help file we learn, "Each patient entering the program was designated officially a heart transplant candidate, meaning that he was gravely ill and would most likely benefit from a new heart." Survival rates are not good for this population, although they are better for those who receive a heart transplant. Do heart transplant recipients still have less than a 50% chance of survival?


## Exploratory data analysis

### Use data documentation (help files, code books, Google, etc.), the `str` command, and other summary functions to understand the data.

[You should type `?heartTr` at the Console to read the help file.]

```{r}
heartTr
```

```{r}
str(heartTr)
```

Commentary: The variable of interest is `survived`, which is coded as a factor variable with two categories, "alive" and "dead". Keep in mind that because we are interested in survival rates, the "alive" condition will be considered the "success" condition.^[Finally, a "success" condition that actually sounds successful!]

**Do not under any circumstances attempt to convert `heartTr$survived` to a factor variable using the `factor` command!** It is already a factor variable. Applying the `factor` command the way you have in past modules will break a variable that is already coded as a factor variable. This is one of the most common mistakes students make when working with factor variables. This is why the `str` command is so important. You always need to check your categorical data to see if it's already a factor variable! (If not, then of course we would recode it the way you've been taught.)

There are 103 patients, but we are not considering all these patients. Our sample should consist of only those patients who actually received the transplant. The following table shows that only 69 patients were in the "treatment" group (meaning that they received a heart transplant).

```{r}
tally(~ transplant, data = heartTr, margins = TRUE)
```

### Prepare the data for analysis.

**CAUTION: If you are copying and pasting from this example to use for another research question, the following code chuck is specific to this research question and not applicable in other contexts.**

We need to use `filter` so we get only the patients who actually received the heart transplant.

```{r}
heartTr2 <- filter(heartTr, transplant == "treatment")
```

Commentary: don't forget the double equal sign (`==`) that checks whether the `treatment` variable is equal to the value "treatment". (See the module `05_Manipulating_data` if you've forgotten how to use `filter`.)

Again, this step isn't something you need to do for other research questions. This question is peculiar because it asks only about patients who received a heart transplant, and that only involves a subset of the data we have in the `heartTr` data frame.

### Make tables or plots to explore the data visually.

Making sure that we refer from now on to the `heartTr2` data frame and not the original `heartTr` data frame:

```{r}
tally(~ survived, data = heartTr2, margins = TRUE)
```

```{r}
tally(~ survived, data = heartTr2, margins = TRUE, format = "percent")
```


## Hypotheses

### Identify the sample (or samples) and a reasonable population (or populations) of interest.

The sample consists of `r NROW(heartTr2)` heart transplant recipients in a study at Stanford University. The population of interest is presumably all heart transplants recipients.

### Express the null and alternative hypotheses as contextually meaningful full sentences.

$H_{0}:$ Heart transplant recipients have a 50% chance of survival.

$H_{A}:$ Heart transplant recipients have less than a 50% chance of survival.

Commentary: it is slightly unusual that we are conducting a one-sided test. The standard default is typically a two-sided test. However, it is not for us to choose: the proposed research question is unequivocal in hypothesizing "less than 50%" survival.

### Express the null and alternative hypotheses in symbols (when possible).

$H_{0}: p_{survival} = 0.5$

$H_{A}: p_{survival} < 0.5$


## Model

### Identify the sampling distribution model.

We will use a normal model.

### Check the relevant conditions to ensure that model assumptions are met.

* Random
    - Since the `r NROW(heartTr2)` patients are from a study at Stanford, we do not have a random sample of all heart transplant recipients. We hope that the patients recruited to this study were physiologically similar to other heart patients so that they are a representative sample. Without more information, we have no real way of knowing.

* 10%
    - `r NROW(heartTr2)` patients are definitely less than 10% of all heart transplant recipients.

* Success/failure

$$np = 69(0.5) = 34.5 \geq 10$$

$$n(1 - p) = 69(0.5) = 34.5 \geq 10$$

Commentary: Notice something interesting here. Why did we not use the 24 patients who survived and the 45 who died as the successes and failures? In other words, why did we use $np$ and $n(1 - p)$ instead of $n \hat{p}$ and $n(1 - \hat{p})$?

Remember the logic of inference and the philosophy of the null hypothesis. To convince the skeptics, we must assume the null hypothesis throughout the process. It's only after we present sufficient evidence that can we reject the null and fall back on the alternative hypothesis that encapsulates our research question.

Therefore, under the assumption of the null, the sampling distribution is the *null distribution*, meaning that it's centered at 0.5. All work we do with the normal model, including checking conditions, must use the null model with $p = 0.5$.

That's also why the numbers don't have to be whole numbers. If the null states that of the 69 patients, 50% are expected to survive, then we expect 50% of 69, or 34.5, to survive. Of course, you can't have half of a survivor. But these are not *actual* survivors. Rather, they are the expected number of survivors in a group of 69 patients *on average* under the assumption of the null.


## Mechanics

### Compute and report the test statistic.

```{r}
surv_test <- binom.test(heartTr2$survived, p = 0.5, success = "alive")
surv_test_tidy <- tidy(surv_test)
surv_test_tidy
```

The test statistic is `r surv_test_tidy$estimate`, the proportion of successes (i.e., survivors) in the sample.

We'll also compute the corresponding z score. First, we compute the standard error:

```{r}
SE1 <- sqrt(0.5*(1 - 0.5)/69)
SE1
```

Then, using our computation of `SE1`, we get the z score.

```{r}
z1 <- (surv_test_tidy$estimate - 0.5)/SE1
z1
```

The z score is `r z1`.

Commentary: We run the `binom.test` command to give us almost everything we need. The argument `p = 0.5` is the null value, and this might be different for a different research question. Also note that the `binom.test` command allows us to specify the "success" condition (here, "alive").

As seen before, the `tidy` command from the `broom` package makes everything neat and gives us easy access to the various pieces of the output. What this command does not give us is a z score. So while we can report the `estimate` ($\hat{p}$) as the test statistic, we should also compute a z score. The `statistic` reported in the output is the raw number of successes, but this number is less useful to us. (See the normal models in the next section.)

To compute the z score, it's easiest to compute the standard error first. The formula is

$$SE = \sqrt{\frac{p(1 - p)}{n}}.$$

We did this in R and stored the result as `SE1`.

**Remember that are working under the assumption of the null hypothesis.** This means that we use $p = 0.5$ everywhere in the formula for the standard error. Then our z score is

$$z = \frac{(\hat{p} - p)}{SE} =  \frac{(\hat{p} - p)}{\sqrt{\frac{p (1 - p)}{n}}} = \frac{(0.348 - 0.5)}{\sqrt{\frac{0.5 (1 - 0.5)}{69}}} =  -2.53.$$

Either $\hat{p}$ or z could be considered the test statistic.^[And the tidy output seems to be considering the raw number of successes (24) as the test statistic.] If we use $\hat{p}$ as the test statistic, then we're considering the null model to be

$$N\left(0.5, \sqrt{\frac{0.5 (1 - 0.5)}{69}}\right).$$

If we use z as the test statistic, then we're considering the null model to be the *standard normal model*:

$$N(0, 1).$$

The standard normal model is more intuitive and easier to work with, both conceptually, and in R. Generally, then, we will consider z as the test statistic so that we can consider our null model to be the standard normal model. For example, knowing that our test statistic is two and a half standard deviations to the left of the null value already tells us a lot. We can anticipate a small P-value leading to rejection of the null.

### Plot the null distribution.

```{r}
pdist("norm", q = z1, invisible = TRUE)
```

Commentary: Notice how simple this `pdist` command is. One reason is that we're using a z score, which means that we are working with the standard normal model. As this is the default for the `pdist` command, there is no need to specify the `mean` and `sd` in the command.

Contrast this to the command we would need if using $\hat{p}$ as our test statistic:

```{r}
pdist("norm", q = surv_test_tidy$estimate,
      mean = 0.5, sd = SE1,
      invisible = TRUE)
```

This works and gives us the same answer, but it's a bit more complicated.

The other thing that makes this relatively simple is that we're running a one-sided test corresponding to the alternative hypothesis $p < 0.5$. Keep in mind that if we were running a two-sided test, we would need this instead:

```{r}
pdist("norm", q = c(-z1, z1), invisible = TRUE)
```

### Calculate and interpret the P-value.

```{r}
P1 <- pdist("norm", q = z1, plot = FALSE)
P1
```

The P-value is `r P1`. If there were truly a 50% chance of survival among heart transplant patients, there would only be a `r 100 * P1`% chance of seeing data at least as extreme as we saw.

Commentary: Since we are running a one-sided test using the left tail (recall the alternative hypothesis was $p < 0.5$), the result of the `pdist` command is already the correct P-value. We don't need the plot again, so we can put `plot = FALSE` into the command.

The P-value can also be computed as part of the `binom.test` function. The problem here is that this is wrong:

```{r}
surv_test_tidy$p.value
```

The `binom.test` function assumes by default that we are performing a two-sided test. Therefore, if we want a one-sided P-value, we have to divide this answer by 2:

```{r}
surv_test_tidy$p.value / 2
```

Even this is a little different from the P-value we computed directly from the normal model. Under the hood, the `binom.test` command is not quite using the same method we're using. Therefore, the P-value is a little different from the one generated from our normal model. Nevertheless, the two methods are close, so you should come to the same conclusion (i.e., reject the null or fail to reject the null).

If you look more deeply into the `binom.test` function, you might discover that there is a way to get it to run a one-sided test. However, doing so messes up the confidence interval that we'll need later in the rubric. Most of the time, it's a moot point because we will often want to run two-sided test anyway.


## Conclusion

### State the statistical conclusion.

We reject the null hypothesis.

### State (but do not overstate) a contextually meaningful conclusion.

We have sufficient evidence that heart transplant recipients have less than a 50% chance of survival.

### Identify the possibility of either a Type I or Type II error and state what making such an error means in the context of the hypotheses.

As we rejected the null, we run the risk of making a Type I error. It is possible that the null is true and that there is a 50% chance of survival for these patients, but we got an unusual sample that appears to have a much smaller chance of survival.


## Confidence interval

### Check the relevant conditions to ensure that model assumptions are met.

* Random
    - Same as above.
* 10%
    - Same as above.
* Success/failure
    - There were 24 patients who survived and 45 who died in our sample. Both are larger than 10.

Commentary: In the "Confidence interval" section of the rubric, there is no need to recheck conditions that have already been checked. The sample has not changed; if it met the "Random" and "10%" conditions before, it will meet them now.

So why recheck the success/failure condition?

Keep in mind that in a hypothesis test, we temporarily assume the null is true. The null states that $p = 0.5$ and the resulting null distribution is, therefore, centered at $p = 0.5$. The success/failure condition is a condition that applies to the normal model we're using, and for a hypothesis test, that's the null model.

By contrast, a confidence interval is making no assumption about the "true" value of $p$. The inferential goal of a confidence interval is to try to capture the true value of $p$, so we certainly cannot make any assumptions about it. Therefore, we go back to the original way we learned about the success/failure condition. That is, we check the actual number of successes and failures.

### Calculate the confidence interval.

```{r}
surv_test_tidy$conf.low
```

```{r}
surv_test_tidy$conf.high
```

Commentary: there's not much to do here as the confidence interval was already computed as a byproduct of the `binom.test` command.

### State (but do not overstate) a contextually meaningful interpretation.

We are 95% confident that the true percentage of heart transplant recipients who survive is captured in the interval (`r 100 * surv_test_tidy$conf.low`%, `r 100 * surv_test_tidy$conf.high`%).

Commentary: Note that when we state our contextually meaningful conclusion, we also convert the decimal proportions to percentages. Humans like percentages a lot better.

### If running a two-sided test, explain how the confidence interval reinforces the conclusion of the hypothesis test.

We are not running a two-sided test, so this step is not applicable.


## Inference using summary statistics

In the previous example, we had access to the actual data frame. In some situations, you are not given the data; rather, all you have are summary statistics about the data. This certainly happens with homework problems from a textbook, but it can happen in "real life" too. If you're reading a research article, you will rarely have access to the original data used in the analysis. All you can see is what the researchers report in their paper. Depending on what kind of information you have, there are a couple of different ways of handling inference.

### Method 1

You may just have a summary of the total number of successes and failures. In our heart transplant example, we had 24 patients who survived and 45 who died. If that's all we know, we can run the `binom.test` command as follows:

```{r}
surv_test_count <- binom.test(24, n = 69)
surv_test_count_tidy <- tidy(surv_test_count)
surv_test_count_tidy
```

Once this is done (in the step "Compute and report the test statistic"), all remaining steps of the rubric stay exactly the same except that you'll use `surv_test_count_tidy` instead of `surv_test_tidy`.

### Method 2

If you are given the percentages of successes and/or failures in your data, you'll have to convert them to whole number totals. You might be told that of 69 patients who received a heart transplant, only 34.8% survived. In that case, we can run the `binom.test` command as follows:

```{r}
surv_test_prop <- binom.test(round(69*0.348), n = 69)
surv_test_prop_tidy <- tidy(surv_test_prop)
surv_test_prop_tidy
```

Once this is done (in the step "Compute the test statistic"), all remaining steps  of the rubric stay exactly the same except that you'll use `surv_test_prop_tidy` instead of `surv_test_tidy`.

If you're paying close attention, you might recognize that we discussed these two methods in a previous module about confidence intervals. (Method 3 in that module was when you had access to the original data.)


## Your turn

Follow the rubric to answer the following research question:

Some heart transplant candidates have already had a prior surgery. Use the variable `prior` in the `heartTr` data set to determine if fewer than 50% of patients have had a prior surgery. (To be clear, you are being asked to perform a one-sided test again.) **Be sure to use the full `heartTr` data, not the modified `heartTr2` from the previous example.**

The rubric outline is reproduced below. You may refer to the worked example above and modify it accordingly. Remember to strip out all the commentary. That is just exposition for your benefit in understanding the steps, but is not meant to form part of the formal inference process.

Another word of warning: the copy/paste process is not a substitute for your brain. You will often need to modify more than just the names of the data frames and variables to adapt the worked examples to your own work. For example, if you run a two-sided test instead of a one-sided test, there are three or four steps that all have to be adjusted accordingly. Understanding the sampling distribution model and the computation of the P-value goes a long way toward understanding the changes that must be made. Do not blindly copy and paste code without understanding what it does. And you should **never** copy and paste text. All the sentences and paragraphs you write are expressions of your own analysis. They must reflect your own understanding of the inferential process.

**Also, so that your answers here don't mess up the code chunks above, use new variable names everywhere. In particular, you should use `prior_test` and `prior_test_tidy` (instead of `surv_test` and `surv_test_tidy`) to store the results of your hypothesis test. Use the following names for your standard error, z score, and P-value: `SE2`, `z2`, and `P2`.**

##### Exploratory data analysis

###### Use data documentation (help files, code books, Google, etc.), the `str` command, and other summary functions to understand the data.

<div class = "answer">

```{r}
# Add code here to understand the data.
```

</div>

###### Prepare the data for analysis. [Not always necessary.]

<div class = "answer">

```{r}
# Add code here to prepare the data for analysis.
```

</div>

###### Make tables or plots to explore the data visually.

<div class = "answer">

```{r}
# Add code here to make tables or plots.
```

</div>


##### Hypotheses

###### Identify the sample (or samples) and a reasonable population (or populations) of interest.

<div class = "answer">

Please write up your answer here.

</div>

###### Express the null and alternative hypotheses as contextually meaningful full sentences.

<div class = "answer">

$H_{0}:$ Null hypothesis goes here.

$H_{A}:$ Alternative hypothesis goes here.

</div>

###### Express the null and alternative hypotheses in symbols (when possible).

<div class = "answer">

$H_{0}: math$

$H_{A}: math$

</div>


##### Model

###### Identify the sampling distribution model.

<div class = "answer">

Please write up your answer here.

</div>

###### Check the relevant conditions to ensure that model assumptions are met.

<div class = "answer">

Please write up your answer here. (Some conditions may require R code as well.)

</div>


##### Mechanics

###### Compute and report the test statistic.

<div class = "answer">

```{r}
# Add code here to compute the test statistic.
```

Please write up your answer here.

</div>

###### Plot the null distribution.

<div class = "answer">

```{r}
# Add code here to plot the null distribution.
```

</div>

###### Calculate and interpret the P-value.

<div class = "answer">

```{r}
# Add code here to calculate the P-value.
```

Please write up your answer here.

</div>


##### Conclusion

###### State the statistical conclusion.

<div class = "answer">

Please write up your answer here.

</div>

###### State (but do not overstate) a contextually meaningful conclusion.

<div class = "answer">

Please write up your answer here.

</div>

###### Identify the possibility of either a Type I or Type II error and state what making such an error means in the context of the hypotheses.

<div class = "answer">

Please write up your answer here.

</div>


##### Confidence interval

###### Check the relevant conditions to ensure that model assumptions are met.

<div class = "answer">

Please write up your answer here. (Some conditions may require R code as well.)

</div>

###### Calculate the confidence interval.

<div class = "answer">

```{r}
# Add code here to calculate the confidence interval.
```

</div>

###### State (but do not overstate) a contextually meaningful interpretation.

<div class = "answer">

Please write up your answer here.

</div>

###### If running a two-sided test, explain how the confidence interval reinforces the conclusion of the hypothesis test.

<div class = "answer">

Please write up your answer here.

</div>
