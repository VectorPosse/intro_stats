---
title: "15. Chi-square goodness-of-fit test"
author: "Put your name here"
date: "Put the date here"
output:
    html_notebook:
        toc: yes
        toc_float: yes
---

<!-- Please don't mess with the next few lines! -->
<style>h5{font-size:2em;color:#0000FF}h6{font-size:1.5em;color:#0000FF}div.answer{margin-left:5%;border:1px solid #0000FF;border-left-width:10px;padding:25px} div.summary{background-color:rgba(30,144,255,0.1);border:3px double #0000FF;padding:25px}</style>`r options(scipen=999)`<p style="color:#ffffff">`r intToUtf8(c(49,46,50))`</p>
<!-- Please don't mess with the previous few lines! -->

<div class = "summary">
### Functions introduced in this module:
`chisq`, `resample`, `chisq.test`, `as.table`
</div>


## Introduction

In this assignment we will learn how to run the chi-square goodness-of-fit test. A chi-square goodness-of-fit test is similar to a test for a single proportion except, instead of two categories (success/failure), we now try to understand the distribution among three or more categories.


## Load packages

We load the standard `mosaic` package and the `openintro` package for the `hsb2` data. The `broom` package will give us tidy output.

```{r, message = FALSE}
library(openintro)
library(broom)
library(mosaic)
```


## Research question

We use a classic data set `mtcars` from a 1974 Motor Trend magazine to examine the distribution of the number of engine cylinders (with values 4, 6, or 8). We'll assume that this data set is representative of all cars from 1974.

In recent years, 4-cylinder vehicles and 6-cylinder vehicles have comprised about 38% of the market each, with nearly all the rest (24%) being 8-cylinder cars. (This ignores a very small number of cars manufactured with 3- or 5-cylinder engines.) Were car engines in 1974 manufactured according to the same distribution?

Here is the structure of the data:

```{r}
str(mtcars)
```

Note that the variable of interest `cyl` is not coded correctly as a factor variable. Let's convert `cyl` to a factor variable first, and put it in its own data frame as we've done many times before. (Since the levels are already called 4, 6, and 8, we do not need to specify `levels` or `labels`.)

```{r}
cyl <- factor(mtcars$cyl)
cyl_df <- data.frame(cyl)
cyl_df
```


## Chi-squared

When we have three or more categories in a categorical variable, it is natural to ask how the observed counts in each category compare to the counts that we expect to see under the assumption of some null hypothesis. In other words, we're assuming that there is some "true" distribution to which we are going to compare our data. Sometimes, this null comes from substantive expert knowledge. (For example, we will be comparing the 1974 distribution to a known distribution from recent years.) Sometimes we're interested to see if our data deviates from a null distribution that predicts an equal number of observations in each category.

First of all, what is the actual distribution of cylinders in our data? Here's a frequency table.
 
```{r}
tally(~ cyl, data = cyl_df)
```

**Very important**: For this module, you must *not* include `margins = TRUE` in the `tally` command. R is not quite smart enough to figure out that the "Total" isn't really part of the data. We also don't want to use `format = "percent"` because the statistical test we will use relies on having the raw counts, not percentages or proportions.

The numbers in the three cells of our frequency table are the "observed" values, usually denoted by the letter $O$.

Since we'll need this table later, we'll go ahead and store it:

```{r}
cyl_tally <- tally(~ cyl, data = cyl_df)
cyl_tally
```


What are the expected counts? Well, since there are 32 cars, we need to multiply 32 by the percentages listed in the research question. For 4-cylinder and 6-cylinder cars, if the distribution of engines in 1974 were the same as today, there would be $32 * 0.38$ or about 12.2 cars we would expect to see in our sample that have 4-cylinder engines, and the same for 6-cylinder cars. For 8-cylinder cars, we expect $32 * 0.24$ or about 7.7 cars in our sample to have 8-cylinder engines. These "expected" counts are usually denoted by the letter $E$.

Why aren't the expected counts whole numbers? In any given data set, of course, we will see a whole number of cars with 4, 6, or 8 cylinders. However, since we're looking only at expected counts, they are the average over lots of possible sets of 32 cars under the assumption of the null. We don't need for these averages to be whole numbers.

How should the deviation between the data and the null distribution be measured? We could simply look at the difference between the observed counts and the expected counts $O - E$. However, there will be some positive values (cells where we have more than the expected number of cars) and some negative values (cells where we have fewer than the expected number of cars). These will all cancel out.

If this sounds vaguely familiar, it is because we encountered the same problem with the formula for the standard deviation. The differences $y - \bar{y}$ had the same issue. Do you recall the solution in that case? It was to square these values, making them all positive.

So instead of $O - E$, we will consider $(O - E)^{2}$. Finally, to make sure that cells with large expected values don't dominate, we divide by $E$:

$$\frac{(O - E)^{2}}{E}.$$

This puts each cell on equal footing. Now that we have a reasonable measure of the deviation between observed and expected counts for each cell, we define $\chi^{2}$ ("chi-squared", pronounced "kye-squared"---rhymes with "die-scared", or if that's too dark, how about "pie-shared"^[Rhyming is fun!]) as the sum of all these fractions, one for each cell:

$$\chi^{2} = \sum \frac{(O - E)^{2}}{E}.$$

A $\chi^{2}$ value of zero would indicate perfect agreement between observed and expected values. As the $\chi^{2}$ value gets larger and larger, this indicates more and more deviation between observed and expected values.

As an example, for our data, we calculate chi-squared as follows:

$$\chi^{2} = \frac{(11 - 12.2)^{2}}{12.2} + \frac{(7 - 12.2)^{2}}{12.2} + \frac{(14 - 7.7)^{2}}{7.7} \approx 7.5.$$

Or we could just do it in R with the `chisq` command. This command does not follow the "formula" notation with the tilde as other commands we've seen. Instead, we have to give it the frequency table that results from applying the `tally` command. (We called this `cyl_tally` above.) In addition, we have to tell `chisq` the proportions that correspond to the null hypothesis. In this case, since the order of entries in the frequency table is 4-cylinder, 6-cylinder, then 8-cylinder, we need to give `chisq` a vector of entries `c(0.38, 0.38, 0.24)` that represents the 38%, 38%, and 24% expected for 4, 6, and 8 cylinders respectively.

```{r}
obs_chisq <- chisq(cyl_tally, p = c(0.38, 0.38, 0.24))
obs_chisq
```


## The chi-square distribution

We know that even if the true distribution were 38%, 38%, 24%, we would not see exactly 12.2, 12.2, 7.7 in a sample of 32 cars. (In fact, the "true" distribution is physically impossible because these are not whole numbers!) So what kinds of numbers could we get?

Let's do a quick simulation to find out. We will use the `resample` command from the `mosaic` package.

First of all, recall the actual distribution of cylinders:

```{r}
tally(~ cyl, data = cyl_df)
```

Under the assumption of the null, there should be a 38%, 38%, and 24% chance of seeing 4, 6, or 8 cylinders, respectively. The `resample` command below takes the values "4", "6", or "8" and grabs them at random according to the probabilities specified until it has 32 values. In other words, it will randomly select "4" about 38% of the time, "6" about 38% of the time, and "8" about 24% of the time, until it gets a list of 32 total cars. But because randomness is involved, the simulated samples are subject to sampling variability.

Here is one simulated data set:

```{r}
set.seed(9090)
tally(resample(c(4, 6, 8), size = 32,
               prob = c(0.38, 0.38, 0.24)))
```

Let's do this a couple more times to see some other possibilities.

```{r}
tally(resample(c(4, 6, 8), size = 32,
               prob = c(0.38, 0.38, 0.24)))
tally(resample(c(4, 6, 8), size = 32,
               prob = c(0.38, 0.38, 0.24)))
tally(resample(c(4, 6, 8), size = 32,
               prob = c(0.38, 0.38, 0.24)))
```

Each table represents 32 randomly sampled cars from a population in which we're assuming a distribution of 38% 4-cylinder cars, 38% 6-cylinder cars, and 24% 8-cylinder cars.

Next, we need to calculate the $\chi^{2}$ value for each sample. This is a simple matter of applying the `chisq` function to each random sample.

```{r}
chisq(tally(resample(c(4, 6, 8), size = 32,
                     prob = c(0.38, 0.38, 0.24))),
      p = c(0.38, 0.38, 0.24))
chisq(tally(resample(c(4, 6, 8), size = 32,
                     prob = c(0.38, 0.38, 0.24))),
      p = c(0.38, 0.38, 0.24))
chisq(tally(resample(c(4, 6, 8), size = 32,
                     prob = c(0.38, 0.38, 0.24))),
      p = c(0.38, 0.38, 0.24))
```

As before, we can use the `do` command to do this a bunch of times and save the results in a data frame.

```{r}
set.seed(9090)
sims <- do(5000) * 
  chisq(tally(resample(c(4, 6, 8), size = 32,
                       prob = c(0.38, 0.38, 0.24))),
        p = c(0.38, 0.38, 0.24))
sims
```

Let's graph the resulting random values of $\chi^{2}$ and include the chi-squared value for our actual data.

```{r}
ggplot(sims, aes(x = X.squared)) +
    geom_histogram(binwidth = 1, boundary =  0) +
    geom_vline(xintercept = obs_chisq, color = "blue")
```

A few things are apparent:

1. The values are all positive. This makes sense when you remember that each piece of the $\chi^{2}$ calculation was positive. This is different from our earlier simulations that looked like normal models. (Z scores can be positive or negative, but not $\chi^{2}$.)

2. This is a severely right-skewed graph. Although most values are near zero, the occasional unusual sample can have a large value of $\chi^{2}$.

3. You can see that our sample (the blue line) is pretty far to the right. It is an unusual value given the assumption of the null hypothesis. In fact, we can count the proportion of sampled values that are to the right of the blue line:

```{r}
prop(sims$X.squared >= obs_chisq)
```

This is the simulated P-value. Keep this number in mind when we calculate the P-value using a sampling distribution model later.


## Chi-square as a sampling distribution model

Just like there was a mathematical model for our simulated data before (the normal model back then), there is also a mathematical model for this type of simulated data. It's called (not surprisingly) the *chi-square distribution*.

There is one new idea, though. Although all normal models have the same bell shape, there are many different chi-square models. This is because the number of cells can change the sampling distribution. Our engine cylinder example has three cells (corresponding to the categories "4", "6", and "8"). But what if there were 10 categories? The shape of the chi-square model would be different.

The terminology used by statisticians to distinguish these models is *degrees of freedom*, abbreviated $df$. The reason for this name and the mathematics behind it are complicated and technical. Suffice it to say for now that if there are $c$ cells, you use $c - 1$ degrees of freedom. For our car example, there are 3 cylinder categories, so $df = 2$.

Look at the graph below that shows the theoretical chi-square models for varying degrees of freedom.

```{r}
# Don't worry about the syntax here.
# You won't need to know how to do this on your own.
ggplot(data.frame(x = c(0, 20)), aes(x)) +
    stat_function(fun = dchisq, args = list(df = 2),
                  aes(color = "2")) +
    stat_function(fun = dchisq, args = list(df = 5),
                  aes(color = "5" )) +
    stat_function(fun = dchisq, args = list(df = 10),
                  aes(color = "10")) +
    scale_color_manual(name = "df",
                       values = c("2" = "red",
                                  "5" = "blue",
                                  "10" = "green"),
                       breaks =  c("2", "5", "10"))
```

The red curve (corresponding to $df = 2$) looks a lot like our simulation above. But as the degrees of freedom increase, the mode shifts further to the right.


## Chi-square goodness-of-fit test

The formal inferential procedure for examining whether data from a categorical variable fits a proposed distribution in the population is called a *chi-square goodness-of-fit test*.

We can use the chi-square model as the sampling distribution as long as the sample size is large enough. This is checked by calculating that the expected cell counts (not the observed cell counts!) are at least 5 in each cell.

We use the `chisq.test` command to run the hypothesis test in R. The `chisq.test` command is like the `chisq` command in that you have to run the test on the frequency table and not the raw data. As usual, we also apply `tidy` to store the output in a tidy fashion.

```{r}
cyl_test <- chisq.test(cyl_tally, p = c(0.38, 0.38, 0.24))
cyl_test_tidy <- tidy(cyl_test)
cyl_test_tidy
```

In addition to the test statistic ($\chi^{2}$) and the P-value, the output also records the degrees of freedom in the `parameter` variable.

We'll walk through the engine cylinder example from top to bottom using the rubric.


## Exploratory data analysis

### Use data documentation (help files, code books, Google, etc.), the `str` command, and other summary functions to understand the data.

[You should type `?mtcars` at the Console to read the help file.]

```{r}
mtcars
```

```{r}
str(mtcars)
```

### Prepare the data for analysis.

```{r}
# Although we've already done this above, 
# we include it here again for completeness.
cyl <- factor(mtcars$cyl)
cyl_df <- data.frame(cyl)
cyl_df
```

### Make tables or plots to explore the data visually.

```{r}
cyl_tally <- tally(~ cyl, data = cyl_df)
cyl_tally
```

Commentary: Again, be sure to save the results of the `tally` command to feed into the `chisq.test` command later. Also be sure *not* to use `margins = TRUE` or `format = "percent"`.


## Hypotheses

### Identify the sample (or samples) and a reasonable population (or populations) of interest.

The sample is a set of `r NROW(cyl_df)` cars from a 1974 Motor Trends magazine. The population is all cars from 1974.

### Express the null and alternative hypotheses as contextually meaningful full sentences.

$H_{0}:$ In 1974, the proportion of cars with 4, 6, and 8 cylinders was 38%, 38%, and 24%, respectively.

$H_{A}:$ In 1974, the proportion of cars with 4, 6, and 8 cylinders was not 38%, 38%, and 24%.

### Express the null and alternative hypotheses in symbols (when possible).

$H_{0}: p_{4} = 0.38, p_{6} = 0.38, p_{8} = 0.24$

There is no easy way to express the alternate hypothesis in symbols because any deviation in any of the categories can lead to rejection of the null. You can't just say $p_{4} \neq 0.38, p_{6} \neq 0.38, p_{8} \neq 0.24$ because one of these categories might have the correct proportion with the other two different and that would still be consistent with the alternative hypothesis.

So the only requirement here is to express the null in symbols.


## Model

### Identify the sampling distribution model.

We use a $\chi^{2}$ model with 2 degrees of freedom.

Commentary: Unlike the normal model, there are infinitely many different $\chi^{2}$ models, so you have to specify the degrees of freedom when you identify it as the sampling distribution model.

### Check the relevant conditions to ensure that model assumptions are met.

* Random
    - We do not know how Motor Trends magazine sampled these 32 cars, so we're not sure if this list is random or representative of all cars from 1974. We should be cautious in our conclusions.

* 10%
    - As long as there are at least 320 different car models, we are okay. This sounds like a lot, so this condition might not quite be met. Again, we need to be careful. (Also note that the population is not all automobiles manufactured in 1974. It is all *types* of automobile manufactured in 1974. There's a big difference.)

* Expected cell counts
    - This condition says that under the null, we should see at least 5 cars in each category. The expected counts are $32(0.38) = 12.2$, $32(0.38) = 12.2$, and $32(0.24) = 7.7$. So this condition is met.


## Mechanics

### Compute the test statistic.

```{r}
cyl_test <- chisq.test(cyl_tally, p = c(0.38, 0.38, 0.24))
cyl_test_tidy <- tidy(cyl_test)
cyl_test_tidy
```

```{r}
cyl_test_tidy$statistic
```

### Report the test statistic in context.

The value of $\chi^{2}$ is `r cyl_test_tidy$statistic`.

Commentary: The $\chi^{2}$ test statistic is, of course, the same value we computed manually by hand earlier. Also, the formula for $\chi^{2}$ is a complicated function of observed and expected values, making it difficult to say anything about this number in the context of cars and engine cylinders. So even though the requirement is to "report the test statistic in context," there's not much one can say here other than just to report the test statistic.

### Plot the null distribution.

```{r}
pdist("chisq", df = cyl_test_tidy$parameter,
      q = cyl_test_tidy$statistic,
      invisible = TRUE)
```

Commentary: We use `pdist`, but now we need to use `"chisq"` instead of `"norm"`. Also, since the chi-square distribution requires the specification of degrees of freedom, there is a new argument to `pdist` called `df`. We could type `df = 2` since we know there are 2 degrees of freedom; however, the degrees of freedom are also stored in the output `cyl_test_tidy` in the `parameter` variable. 

### Calculate the P-value.

```{r}
P1 <- 1 - pdist("chisq", df = cyl_test_tidy$parameter,
                q = cyl_test_tidy$statistic,
                plot = FALSE)
P1
```

The P-value is also stored in the tidy output:

```{r}
cyl_test_tidy$p.value
```

Commentary: Values that are as extreme or even more extreme than the test statistic are in the right tail. If we use `pdist`, remember that it always shades to the left by default, so we have to subtract the output from 1 to get the correct P-value. Also remember to add `plot = FALSE` as we don't really need to look at the same picture again.


### Interpret the P-value as a probability given the null.

The P-value is `r P1`. If the true distribution of cars in 1974 were 38% 4-cylinder, 38% 6-cylinder, and 24% 8-cylinder, there would be a `r 100 * P1`% chance of seeing data at least as extreme as what we saw.


## Conclusion

### State the statistical conclusion.

We reject the null.

### State (but do not overstate) a contextually meaningful conclusion.

There is sufficient evidence that in 1974, the distribution of cars was not 38% 4-cylinder, 38% 6-cylinder, and 24% 8-cylinder.

### Identify the possibility of either a Type I or Type II error and state what making such an error means in the context of the hypotheses.

If we made a Type I error, that would mean the true distribution of cars in 1974 was 38% 4-cylinder, 38% 6-cylinder, and 24% 8-cylinder, but our sample showed otherwise.


## Confidence interval

There is no confidence interval for a chi-square test. Since our test is not about measuring some parameter of interest (like $p$ or $p_{1} - p_{2}$), there is no interval to produce.

However we will perform a different kind of analysis...


## Post hoc analysis

When we reject the null, we are left with a very vague alternative hypothesis: there is some difference somewhere in one or more categories. Often, we want to follow up to figure out which categories are the ones deviating from the null expectation.

The best way to do this is to look at the *residuals*. A residual for a cell measures how far away the observed count is from the expected count. It does no good just to calculate $O - E$ however; cells with a large count may be far away from their expected values only because they are large numbers. What we want is some kind of relative distance.

We could use the chi-square component from each cell; in other words, we could look at

$$\frac{(O- E)^{2}}{E}.$$

It is more traditional in statistics to look at the square root of this quantity:

$$\frac{(O - E)}{\sqrt{E}}.$$

Additionally, the above quantity can be positive or negative, and that gives us more information about the direction in which there is a deviation.

If you fail to reject the null, you don't have any evidence of a difference anywhere, and so there's not much point in examining the residuals. Here, though, we did reject the null, so it's worth studying them.

The residuals are not stored as part of the `tidy` output, so we'll have to grab them from the original `cyl_test` object:

```{r}
cyl_test$residuals
```

These numbers don't mean anything in absolute terms; they are only interpretable relative to each other. For example, the first residual is negative, but tiny compared to the others. This means that the observed number of 4-cylinder cars is very close to the expected number. On the other hand, the number of observed 6-cylinder cars is much less than expected, whereas the number of observed 8-cylinder cars is much more than expected. If you go back to the table we made earlier, you can verify that.


## Uniform null hypotheses

Suppose we expected an equal number of 4, 6, and 8 cylinder models. This represents a different null hypothesis. While this type of null is not appropriate in every situation, when it does apply, the code gets a lot more simple.

Suppose that we expected an equal number of 4, 6, and 8 cylinder models. We would run the test without having to specify `p`, the vector of proportions:

```{r}
cyl_test2 <- chisq.test(cyl_tally)
```

Once you've run the `chisq.test` command, you can see the expected counts in each category.

```{r}
cyl_test2$expected
```

(Again, we have to use the plain `chisq.test` results without `tidy`.)

You can check on your own that 10.66667 is 1/3 of 32.


## Non-uniform null hypotheses expressed as fractions

The numbers defining the null have to add up to 1 (since they are proportions). This causes some trouble, for example, when you have percentages with an infinite number of decimal places. For example, what if we expected under the null a distribution of 6/13, 4/13, 3/13? You can't express any of these as a decimal without rounding. Well it turns out that `chisq.test` can handle any set of numbers as long as you set `rescale.p = TRUE`:

```{r}
cyl_test3 <- chisq.test(cyl_tally, p = c(6, 4, 3),
                        rescale.p = TRUE)
tidy(cyl_test3)
```


## Inference using a frequency table

In the previous example, we had access to the actual data frame. In some situations, you are not given the data; rather, all you have is a frequency table of the data. This certainly happens with homework problems from a textbook, but it can happen in "real life" too. If you're reading a research article, you will rarely have access to the original data used in the analysis. All you can see is what the researchers report in their paper. 

Suppose all we know is the distribution of cylinders among the 32 cars. Since the `chisq.test` command requires a table as input, we'll have to manually input the numbers 11, 7, and 14 into a table.

The `as.table` command below converts a vector of values to a table.

```{r}
cyl_table <- as.table(c(11, 7, 14))
cyl_table
```

There is a way to change the column names to something more informative than "A", "B", and "C", but it's not important. The goal is to create a quick-and-dirty table just for purposes of getting `chisq.test` to work.

Now we use `chisq_test` as before.

```{r}
cyl_test_manual <- chisq.test(cyl_table,
                              p = c(0.38, 0.38, 0.24))
cyl_test_manual_tidy <- tidy(cyl_test_manual)
cyl_test_manual_tidy
```

Once this is done (in the step "Compute the test statistic"), all remaining steps of the rubric stay exactly the same except that you'll use `cyl_test_manual_tidy` instead of `cyl_test_tidy`.


## Your turn

Use the `hsb2` data and determine if the proportion of high school students who attend general programs, academic programs, and vocational programs is 15%, 60%, and 25% respectively.

The rubric outline is reproduced below. You may refer to the worked example above and modify it accordingly. Remember to strip out all the commentary. That is just exposition for your benefit in understanding the steps, but is not meant to form part of the formal inference process.

Another word of warning: the copy/paste process is not a substitute for your brain. You will often need to modify more than just the names of the data frames and variables to adapt the worked examples to your own work. Do not blindly copy and paste code without understanding what it does. And you should **never** copy and paste text. All the sentences and paragraphs you write are expressions of your own analysis. They must reflect your own understanding of the inferential process.

**Also, so that your answers here don't mess up the code chunks above, use new variable names everywhere.**

If you reject the null, run a post hoc analysis and comment on the cells that seem to be contributing the most to the discrepancy between observed and expected counts.


##### Exploratory data analysis

###### Use data documentation (help files, code books, Google, etc.), the `str` command, and other summary functions to understand the data.

<div class = "answer">

```{r}
# Add code here to understand the data.
```

</div>

###### Prepare the data for analysis. [Not always necessary.]

<div class = "answer">

```{r}
# Add code here to prepare the data for analysis.
```

</div>

###### Make tables or plots to explore the data visually.

<div class = "answer">

```{r}
# Add code here to make tables or plots.
```

</div>


##### Hypotheses

###### Identify the sample (or samples) and a reasonable population (or populations) of interest.

<div class = "answer">

Please write up your answer here.

</div>

###### Express the null and alternative hypotheses as contextually meaningful full sentences.

<div class = "answer">

$H_{0}:$ Null hypothesis goes here.

$H_{A}:$ Alternative hypothesis goes here.

</div>

###### Express the null and alternative hypotheses in symbols (when possible).

<div class = "answer">

$H_{0}: math$

$H_{A}: math$

</div>


##### Model

###### Identify the sampling distribution model.

<div class = "answer">

Please write up your answer here.

</div>

###### Check the relevant conditions to ensure that model assumptions are met.

<div class = "answer">

Please write up your answer here. (Some conditions may require R code as well.)

</div>


##### Mechanics

###### Compute the test statistic.

<div class = "answer">

```{r}
# Add code here to compute the test statistic.
```

</div>

###### Report the test statistic in context.

<div class = "answer">

Please write up your answer here.

</div>

###### Plot the null distribution.

<div class = "answer">

```{r}
# Add code here to plot the null distribution.
```

</div>

###### Calculate the P-value.

<div class = "answer">

```{r}
# Add code here to calculate the P-value.
```

</div>

###### Interpret the P-value as a probability given the null.

<div class = "answer">

Please write up your answer here.

</div>


##### Conclusion

###### State the statistical conclusion.

<div class = "answer">

Please write up your answer here.

</div>

###### State (but do not overstate) a contextually meaningful conclusion.

<div class = "answer">

Please write up your answer here.

</div>

###### Identify the possibility of either a Type I or Type II error and state what making such an error means in the context of the hypotheses.

<div class = "answer">

Please write up your answer here.

</div>


##### Post-hoc analysis (if null was rejected)

You only need to complete the following section if the null was rejected above.

<div class = "answer">

```{r}
# Add code here to calculate the residuals
```

Please write up your answer here.

</div>
