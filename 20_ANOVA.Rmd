---
title: "20. ANOVA"
author: "Put your name here"
date: "Put the date here"
output:
    html_notebook:
        toc: yes
        toc_float: yes
---

<!-- Please don't mess with the next few lines! -->
<style>h5{font-size:2em;color:#0000FF}h6{font-size:1.5em;color:#0000FF}div.answer{margin-left:5%;border:1px solid #1E90FF;border-left-width:10px;padding:25px} div.summary{background-color:rgba(30,144,255,0.1);border:3px double #0000FF;padding:25px}</style><p style="color:#ffffff">`r options(scipen = 999);intToUtf8(c(49,46,48))`</p>
<!-- Please don't mess with the previous few lines! -->

<div class = "summary">
### Functions introduced in this module:
`aov`, `TukeyHSD`
</div>


## Introduction

ANOVA stands for "Analysis of Variance". In this module, we will study the most basic form of ANOVA, called "one-way ANOVA". We've already considered the one-sample and two-sample t tests for means. ANOVA is what you do when you want to compare means for three or more groups.


## Load packages

We load the standard `mosaic` package and the `quantreg` package for the `uis` data. The `broom` package gives us tidy output.

```{r, message = FALSE}
library(quantreg)
data(uis)
library(broom)
library(mosaic)
```


## Research question

The `uis` dataset from the `quantreg` package contains data from the UIS Drug Treatment Study. Is a history of IV drug use associated with depression?


## Data preparation and exploration

Let's look at the UIS data structure:

```{r}
str(uis)
```

To talk about the ANOVA procedure, we'll use the `BECK` and `IV` variables. We need to convert `IV` to a factor variable first (using the help file for guidance). Then we'll put both variables into a new data frame for convenience.

```{r}
IV <- factor(uis$IV, levels = c(1, 2, 3),
             labels = c("Never", "Previous", "Recent"))
BECK_IV <- data.frame(BECK = uis$BECK, IV)
BECK_IV
```

Let's look at the three groups in our data defined by the `IV` variable. These are people who have never used IV drugs, those who have previously used IV drugs, and those who have recently used IV drugs. The following table shows how many people are in each group.

```{r}
tally(~ IV, data = BECK_IV)
```

We're interested in depression as measured by the Beck Depression Inventory.

##### Exercise 1

Google the Beck Depression Inventory. Write a short paragraph about it and how it purports to measure depression.

<div class = "answer">

Please write up your answer here.

</div>

*****

A useful graph is a side-by-side boxplot.

```{r}
ggplot(BECK_IV, aes(y = BECK, x = IV)) +
    geom_boxplot()
```

This boxplot shows that the distribution of depression scores is similar across the groups. There are some small differences, but it's not clear if these differences are statistically significant.

The mean Beck score is calculated with the `mean` command. If we don't use the tilde, we'll just get one overall mean for the whole sample, often called the "grand mean":

```{r}
mean(BECK_IV$BECK)
```

But if we use the tilde, we can separate this out by `IV` group:

```{r}
mean(BECK ~ IV, data = BECK_IV)
```


## The F distribution

When assessing the differences among groups, there are two numbers that are important.

The first is called the "mean square between groups" (MSG). It measures how far away each group mean is away from the overall grand mean for the whole sample. For example, for those who never used IV drugs, their mean Beck score was 15.95. This is 1.42 points below the grand mean of 17.37. On the other hand, recent IV drug users had a mean Beck score of nearly 19. This is 1.63 points above the grand mean. MSG is calculated by taking these differences for each group, squaring them to make them positive, weighting them by the sizes of each group (larger groups should obviously count for more), and dividing by the "group degrees of freedom" $df_{G} = k - 1$ where $k$ is the number of groups. The idea is that MSG is a kind of "average variability" among the groups. In other words, how far away are the groups from the grand mean (and therefore, from each other)?

The second number of interest is the "mean square error" (MSE). It is a measure of variability within groups. In other words, it measures how far away data points are from their own group means. Even under the assumption of a null hypothesis that says all the groups should be the same, we still expect some variability. Its calculation also involves dividing by some degrees of freedom, but now it is $df_{E} = n - k$.

All that is somewhat technical and complicated. We'll leave it to the computer. The key insight comes from considering the ratio of $MSG$ and $MSE$. We will call this quantity F:

$$F = \frac{MSG}{MSE}.$$

What can be said about this magical F? Under the assumption of the null hypothesis, we expect some variability among the groups, and we expect some variability within each group as well, but these two sources of variability should be about the same. In other words, $MSG$ should be roughly equal to $MSE$. Therefore, F ought to be close to 1.

It's not particularly interesting if F is less than one. That just means that the variability between groups is small and the variability of the data within each group is large. That doesn't allow us to conclude that there is a difference among groups. However, if F is really large, that means that there is much more variability between the groups than there is within each group. Therefore, the groups are far apart and there is evidence of a difference among groups.

$MSG$ and $MSE$ are measures of variability, and that's why this is called "Analysis of Variance".

The F distribution is the correct sampling distribution model. Like a t model, there are infinitely many different F models because degrees of freedom are involved. But unlike a t model, the F model has *two* numbers called degrees of freedom, $df_{G}$ and $df_{E}$. Both of these numbers affect the precise shape of the F distribution.

For example, here is picture of a few different F models.

```{r}
# Don't worry about the syntax here.
# You won't need to know how to do this on your own.
ggplot(data.frame(x = c(0, 5)), aes(x)) +
    stat_function(fun = df, args = list(df1 = 1, df2 = 1),
                  aes(color = "1, 1")) +
    stat_function(fun = df, args = list(df1 = 5, df2 = 2),
                  aes(color = "5, 2" )) +
    stat_function(fun = df, args = list(df1 = 50, df2 = 50),
                  aes(color = "50, 50")) +
    scale_color_manual(name = expression(paste(df[G], ", ", df[E])),
                       values = c("1, 1" = "red",
                                  "5, 2" = "blue",
                                  "50, 50" = "green"),
                       breaks =  c("1, 1", "5, 2", "50, 50"))
```


## Assumptions

What conditions can we check to justify the use of an F model for our sampling distribution? In addition to the typical "Random" and "10%" conditions that ensure independence, we also need to check the "Nearly normal" condition for each group, just like for the t tests. A new assumption is the "Constant variance" assumption, which says that each group should have the same variance in the population. This is impossible to check, although we can use our sample as a rough guide. If each group has about the same spread, that is some evidence that such an assumption might hold in the population as well. Also, ANOVA is pretty robust to this assumption, especially when the groups are close to the same size. Even when the group sizes are unequal (sometimes called "unbalanced"), some say the variances can be off by up to a factor of 3 and ANOVA will still work pretty well. So what we're looking for here are gross violations, not minor ones.

Let's go through the rubric with commentary.


## Exploratory data analysis

### Use data documentation (help files, code books, Google, etc.), the `str` command, and other summary functions to understand the data.

[You should type `?uis` at the Console to read the help file. You have already googled the Beck Depression Score.]

```{r}
uis
```

```{r}
str(uis)
```

We can also use `favstats` to explore the `BECK` variable grouped by `IV`.

```{r}
favstats(BECK ~ IV, data = uis)
```

### Prepare the data for analysis. [Not always necessary.]

We need `IV` to be a factor variable. We put `BECK` and `IV` into a separate data frame.

```{r}
# Although we've already done this above, 
# we include it here again for completeness.
IV <- factor(uis$IV, levels = c(1, 2, 3),
             labels = c("Never", "Previous", "Recent"))
BECK_IV <- data.frame(BECK = uis$BECK, IV)
BECK_IV
```

### Make tables or plots to explore the data visually.

The following table shows how many people are in each group.

```{r}
tally(~ IV, data = BECK_IV)
```

Here are two graphs that are appropriate for one categorical and one numerical variable: a side-by-side boxplot and a stacked histogram.

```{r}
ggplot(BECK_IV, aes(y = BECK, x = IV)) +
    geom_boxplot()
```

```{r}
ggplot(BECK_IV, aes(x = BECK)) +
    geom_histogram(binwidth = 5, boundary = 0) +
    facet_grid(IV ~ .)
```

Both graphs show that the distribution of depression scores in each group is similar.

The distributions look reasonably normal, or perhaps a bit right skewed, but we can also check the QQ plots:

```{r}
ggplot(BECK_IV, aes(sample = BECK)) +
    geom_qq() +
    facet_grid(IV ~ .)
```

There is one outlier in the "Previous" group, but with sample sizes as large as we have in each group, it's unlikely that this outlier will be influential. So we'll just leave it in the data and not worry about it.

Here are the group means:

```{r}
mean(BECK ~ IV, data = BECK_IV)
```


## Hypotheses

### Identify the sample (or samples) and a reasonable population (or populations) of interest.

The sample consists of people who participated in the UIS drug treatment study. Because the UIS studied the effects of residential treatment for drug abuse, the population is, presumably, all drug addicts. (Having said that, the help file is not particularly helpful for figuring out who was in this study.)

### Express the null and alternative hypotheses as contextually meaningful full sentences.

$H_{0}:$ There is no difference in depression levels among those who have no history of IV drug use, those who have some previous IV drug use, and those who have recent IV drug use.

$H_{A}:$ There is a difference in depression levels among those who have no history of IV drug use, those who have some previous IV drug use, and those who have recent IV drug use.

### Express the null and alternative hypotheses in symbols (when possible).

$H_{0}: \mu_{never} = \mu_{previous} = \mu_{recent}$

There is no easy way to express the alternate hypothesis in symbols because any deviation in any of the categories can lead to rejection of the null. You can't just say $\mu_{never} \neq \mu_{previous} \neq \mu_{recent}$ because two of these categories might be the same and the third different and that would still be consistent with the alternative hypothesis.

So the only requirement here is to express the null in symbols.


## Model

### Identify the sampling distribution model.

We will use an F model with $df_{G} = 2$ and $df_{E} = 572$.

Commentary: Remember that

$$df_{G} = k - 1 = 3 - 1 = 2,$$

($k$ is the number of groups, in this case, 3), and

$$df_{E} = n - k = 575 - 3 = 572.$$

### Check the relevant conditions to ensure that model assumptions are met.

* Random
    - We have no information about how this sample was collected, so we have to hope it's representative.

* 10%
    - `r NROW(uis)` is definitely less than 10% of all drug addicts.

* Nearly normal
    - The earlier stacked histograms and QQ plots showed that each group is nearly normal. (There was one outlier in one group, but our sample sizes are quite large.)
    
* Constant variance
    - The spread of data looks pretty consistent from group to group in the stacked histogram and side-by-side boxplot.


## Mechanics

### Compute and report the test statistic.

```{r}
BECK_test <- aov(BECK ~ IV, data = BECK_IV)
BECK_test_tidy <- tidy(BECK_test)
BECK_test_tidy
```

```{r}
F1 <- BECK_test_tidy$statistic[1]
F1
```

The F score is `r F1`.

Commentary: ANOVA is run with the `aov` command and the tilde notation. (`BECK ~ IV` means, "Calculate the means of BECK grouped by IV.")

This is the first example we've seen where the tidy output of a hypothesis test has had more than one row. The table of values here is called an ANOVA table. Although it looks foreign, you actually know what almost all of these numbers mean, although by different names.

The first row contains everything related to the groups (as indicated by the `IV` term). The 2 degrees of freedom listed here are what we were calling $df_{G}$. The next number, `sumsq`, or the "sum of squares", is just adding up all the squared differences between the group means and the grand mean, weighted by the size of each group. You can easily see that this sum (1148.039) divided by the degrees of freedom (2) gives you `meansq` (574.01934) which we were calling $MSG$.

The second row says `Residuals`, which is just a technical term for the distances between the data points and their group means. We have seen the number 572 before; this is $df_{E}$. The sum of squares is now measuring the squared distance from each data value to its group mean, the same idea as above but for data points instead of whole groups. Again, check for yourself that `sumsq` (48849.766) divided by the degrees of freedom (572) gives you `meansq` (85.40169), which is just $MSE$.

Moving to the right in the table, what is the `statistic`? This is the value of F. Recall that

$$F = \frac{MSG}{MSE}.$$

Check for yourself: 

```{r}
574.01934/85.40169
```

If we need to isolate this number, it no longer works simply to type `BECK_test_tidy$statistic`:

```{r}
BECK_test_tidy$statistic
```

There are two entries here because the tidy output has two rows, but there is only one F score. (`NA` is the R code for a missing value.) We can grab the first entry like this:

```{r}
BECK_test_tidy$statistic[1]
```


### Plot the null distribution.

```{r}
pdist("f", df1 = BECK_test_tidy$df[1], df2 = BECK_test_tidy$df[2],
      q = BECK_test_tidy$statistic[1],
      invisible = TRUE)
```

Commentary: As the F distribution has two parameters corresponding to $df_{G}$ and $df_{E}$, we have to feed both of them into the `pdist` command. The `pdist` uses `df1` to refer to $df_{G}$ and `df2` to refer to $df_{E}$.  As before, the tidy output has two numbers in the $df$ column, so we have to use `[1]` and `[2]` to grab them.

### Calculate and interpret the P-value.

```{r}
P1 <- BECK_test_tidy$p.value[1]
P1
```

The P-value is `r P1`. If there were no differences in depression scores among the three IV groups, there would be a `r 100 * P1`% chance of seeing data at least as extreme as the data we saw.

Commentary: The issue here is the same as in the last step. The P-value is located in the first entry of `BECK_test_tidy$p.value`, so we need to append `[1]` to the variable to grab only the first value listed.

Note that this is, by definition, a one-sided test. Extreme values of F are the ones that are far away from 1, and only those values in the right tail are far from 1.

We obtain the same answer directly from the F distribution using `pdist`.

```{r}
1 - pdist("f", df1 = BECK_test_tidy$df[1], df2 = BECK_test_tidy$df[2],
      q = BECK_test_tidy$statistic[1],
      plot = FALSE)
```


## Conclusion

### State the statistical conclusion.

We reject the null hypothesis.

### State (but do not overstate) a contextually meaningful conclusion.

There is sufficient evidence that there is a difference in depression levels among those who have no history of IV drug use, those who have some previous IV drug use, and those who have recent IV drug use.

### Identify the possibility of either a Type I or Type II error and state what making such an error means in the context of the hypotheses.

If we've made a Type I error, that means that there really isn't a difference among the three groups, but our sample is an unusual one that did detect a difference.

*****

##### Exercise 2(a)

Everything we saw earlier in the exploratory data analysis pointed toward failing to reject the null. All three groups look very similar in all the plots, and the means are not all that far from each other. So why did we get such a tiny P-value and reject the null? In other words, what is it about our data that allows for small effects to be statistically significant?

<div class = "answer">

Please write up your answer here.

</div>

##### Exercise 2(b)

If you were a psychologist working with drug addicts, would the statistical conclusion (rejecting the null and concluding that there was a difference among groups) be of clinical importance to you? In other words, if there is a difference, is it of practical significance and not just statistical significance?

<div class = "answer">

Please write up your answer here.

</div>

*****

There is not really a confidence interval for ANOVA. We are not hypothesizing about the value of any particular parameter, so there's nothing to estimate with a confidence interval. However, there is something of interest to estimate. That leads us to the...


## Post hoc analysis

If it has been determined that there is a difference among groups, it's natural to ask which group or groups are most different from the others. For example, we could have ten groups, nine that are identical, and one that is radically different. Rejecting the null means we have evidence of a difference overall, but it doesn't specify where the difference lies. Wouldn't it be nice to know which group or groups were different enough to trigger a statistically significant result?

One easy way to do this is to compute Tukey Honest Significant Differences with the `TukeyHSD` command.^[John Tukey was a famous statistician, not to be confused with John Turkey, the Thanksgiving cousin of David S. Pumpkins.]

The `TukeyHSD` command doesn't like the tidy output; it wants the raw output from the `aov` command, stored in `BECK_test`. So let's give Tukey what he wants. We don't want to make Tukey angry.

```{r}
TukeyHSD(BECK_test)
```

This command computes "pairwise differences" which means that it compares every possible pair of groups. For example, the first row indicates that the difference in means between the "Previous" group and the "Never" group is 0.692054. That number is somewhat meaningless by itself, though. Following it is a confidence interval for the "true" difference in means between the "Previous" and "Never" groups. Since this interval contains zero, we don't have any evidence of a difference between "Previous" and "Never". This is also reflected in a large P-value on the right side of the table.

Contrast the first row to the second row of the table, the difference between "Recent" and "Never". Here the difference between the means, 3.043674, is much larger. Furthermore, the confidence interval completely misses zero and the P-value is very small. There is a significant difference between the "Recent" and "Never" groups. (As referenced in an exercise above, though, don't confuse statistical significance for practical significance or clinical significance.)

The last row shows a difference that is also a bit on the larger side, but the interval does contain zero and the P-value is not quite small enough by the traditional $\alpha = 0.05$ standard.

So it seems that it's the "Recent" group that is somewhat different from the other two.

All that's really happening here is that the `TukeyHSD` command is running two-sample t tests between each pair of categories, coming up with both a confidence interval and a P-value.

You may have wondered what `p adj` means. These are "adjusted" P-values. Why do they need to be adjusted? Well, if $\alpha = 0.05$, that means that we have a 5% chance of making a Type I error and saying something is significant even when it's not. This is a 1 in 20 chance. If we then run 20 t tests, chances are that one of them will come back "significant" even though it isn't. That's just how the laws of probability work.

Now, of course, we're not running 20 tests here; we're running three. Nevertheless, every time we run an additional test on the same data, we increase the likelihood of a Type I error. One way to counteract that is to inflate the P-values a little so that only the really, really significant differences get flagged as significant. And even though it doesn't say it, the confidence intervals are widened a bit as well.

I'm sweeping a lot of details under the rug. A more advanced statistics class will deal directly with this issue of "multiple comparisons" and how to adjust for them.


## Your turn

Use the same data. This time determine if depression is associated with heroine/cocaine use during the three months prior to admission. Run ANOVA according to the rubric and then perform a post hoc analysis using Tukey Honest Significant Differences.

The rubric outline is reproduced below. You may refer to the worked example above and modify it accordingly. Remember to strip out all the commentary. That is just exposition for your benefit in understanding the steps, but is not meant to form part of the formal inference process.

Another word of warning: the copy/paste process is not a substitute for your brain. You will often need to modify more than just the names of the data frames and variables to adapt the worked examples to your own work. Do not blindly copy and paste code without understanding what it does. And you should **never** copy and paste text. All the sentences and paragraphs you write are expressions of your own analysis. They must reflect your own understanding of the inferential process.

**Also, so that your answers here don't mess up the code chunks above, use new variable names everywhere.**


##### Exploratory data analysis

###### Use data documentation (help files, code books, Google, etc.), the `str` command, and other summary functions to understand the data.

<div class = "answer">

```{r}
# Add code here to understand the data.
```

</div>

###### Prepare the data for analysis. [Not always necessary.]

<div class = "answer">

```{r}
# Add code here to prepare the data for analysis.
```

</div>

###### Make tables or plots to explore the data visually.

<div class = "answer">

```{r}
# Add code here to make tables or plots.
```

</div>


##### Hypotheses

###### Identify the sample (or samples) and a reasonable population (or populations) of interest.

<div class = "answer">

Please write up your answer here.

</div>

###### Express the null and alternative hypotheses as contextually meaningful full sentences.

<div class = "answer">

$H_{0}:$ Null hypothesis goes here.

$H_{A}:$ Alternative hypothesis goes here.

</div>

###### Express the null and alternative hypotheses in symbols (when possible).

<div class = "answer">

$H_{0}: math$

$H_{A}: math$

</div>


##### Model

###### Identify the sampling distribution model.

<div class = "answer">

Please write up your answer here.

</div>

###### Check the relevant conditions to ensure that model assumptions are met.

<div class = "answer">

Please write up your answer here. (Some conditions may require R code as well.)

</div>


##### Mechanics

###### Compute and report the test statistic.

<div class = "answer">

```{r}
# Add code here to compute the test statistic.
```

Please write up your answer here.

</div>

###### Plot the null distribution.

<div class = "answer">

```{r}
# Add code here to plot the null distribution.
```

</div>

###### Calculate and interpret the P-value.

<div class = "answer">

```{r}
# Add code here to calculate the P-value.
```

Please write up your answer here.

</div>


##### Conclusion

###### State the statistical conclusion.

<div class = "answer">

Please write up your answer here.

</div>

###### State (but do not overstate) a contextually meaningful conclusion.

<div class = "answer">

Please write up your answer here.

</div>

###### Identify the possibility of either a Type I or Type II error and state what making such an error means in the context of the hypotheses.

<div class = "answer">

Please write up your answer here.

</div>


##### Post hoc analysis

<div class = "answer">

```{r}
# Add code here to compute Tukey Honest Significant Differences
```

Please write up your answer here.

</div>
